<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Multi-modal Multi-channel Speech Separation</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<!-- Banner -->
								<section id="banner">
									<div class="content">
										<header>
											<h2>Multi-modal Multi-channel Speech Separation</h2>
											<p>Authors: Rongzhi Gu, Shi-Xiong Zhang, Lianwu Chen, Yong Xu, Yuexian Zou, Dong Yu</p>
										</header>
										<p>Abstract:
										 Target speech separation refers to extracting a target speaker's voice from overlapped audios of simultaneous talkers.
        Previously the use of visual clue for target speech separation has shown great potentials.
        This paper extends it with a general multi-modal framework in the far-field scenario.
        The framework leverages all the available information of target speaker, including his/her spatial location, voice characteristics and lip movements.
        These target-related features are extracted and learned together with the separation task in an end-to-end neural architecture.
        An important aspect of the multi-modal joint modeling is the form of fusion.
        A factorized attention-based fusion method is proposed to aggregate different modalities.
        To validate the robustness of proposed multi-modal system in practice, we evaluate it under different challenging conditions, such as one of the modalities is temporarily missing, invalid or noisy.
        Experiments are carried out on a large-scale audio-visual dataset collected from YouTube and simulated into multichannels.
        Evaluation results illustrate that our proposed multi-modal framework significantly outperforms single-modal speech separation approaches, even when the target's location or lip information is temporally missing or corrupted.
										</p>
										<!--
										<ul class="actions">
											<li><a href="#" class="button big">Learn More</a></li>
										</ul>-->
									</div>
									<span class="image object" style="background-size: contact; background-repeat: no-repeat">
										<img src="images/diagram-20191008.png" style="background-repeat:no-repeat; background-size:contact" width="50%" alt=""/>
									</span>
								</section>

							<!-- Section -->
								<section>
									<header class="major">
										<h2>Details</h2>
									</header>
									<div class="features">
										<article>
											<div class="content">
												<h3>Dataset</h3>
												<p>The audio-visual corpus used for experiments is collected from Youtube, in which Mandarin accounts for the vast majority.</p>
												<p>The simulated dataset contains 160,000, 15,000 and 1,200 multi-channel noisy and reverberant mixtures for
          training, validation and testing. The speakers in the training set and test set are not overlapped, which means our approach is evaluated under speaker-independent scenario.</p>
											</div>
										</article>
										<article>
											<div class="content">
												<h3>Simulation</h3>
												<p>We use a 9-element non-uniform linear array, with spacing 4-3-2-1-1-2-3-4 cm.
          The Multi-channel audio signals are generated by convolving single-channel signals with RIRs simulated by image-source method (ISM) \cite{ISM} .</p>
												<p>We consider three scenarios for the synthetic examples generation: 1 speaker, 2 speakers and 3 speakers,
          respectively accounts for 49%, 30% and 21% in the test dataset.</p>
											</div>
										</article>
										<article>
											<div class="content">
												<h3>Framework</h3>
												<p>As shown in figure above, the proposed system is a multi-stream architecture which takes four inputs: 
												(i) noisy multi-channel mixture waveforms, 
												(ii) target speaker's direction calculated by face detection, 
												(iii) video frames of cropped lip regions, 
												(iv) enrollment audio(s) of the target speaker. 
												The system directly outputs estimated monaural target speech, 
												while all other interfering signals are suppressed.  </p>
											</div>
										</article>
										<article>
											<div class="content">
												<h3>Features</h3>
												<p>Audio: 1) Speaker-independent features, including spectral feature (logrithm power spectrum(LPS)) and spatial features (inter-channel phase difference, IPD)).
												2) Speaker-related feature: a directional feature computed with the target speaker's direction. </p>
												<p>Video: The video stream takes gray image frames of the target speaker's lip region. The structure of lip reading network is similar to the one proposed by \cite{afouras2018conversation}.
												It consists of a spatio-temporal convolution layer and a 18-layer ResNet, followed by several video blocks. The output of the video blocks are target speaker's lip embeddings. </p>
												<p>Speaker Embedding: The speaker model was pretrained on speaker verification task, The input to the speaker model is an enrollment utterance of the target speaker. 
												The speaker model outputs the utterance-level speaker embedding. </p>
											</div>
										</article>
									</div>
								</section>

							<!-- Section -->
								<section>
									<header class="major">
										<h2>A. 1-speaker samples</h2>
									</header>
									<div class="posts">
										<article>
											<a href="#" class="image"><img src="images/pic01.jpg" alt="" /></a>
											<h3>sample1</h3>
											
											
											<table>
												<thead>
													<tr>
														<th> Mixture (1st channel)</th>
														<th> audio-only</th>
														<th> audio-visual</th>
														<th> multi-modal</th>
													</tr>
												</thead>
												<tbody>
													<tr style="border-top:1px solid black">
													<td rowspan="2"><audio controls class="audio-player" preload="metadata" style="width:180px;">
												</tbody>
											</table>
											<p>detail</p>
											<!--<ul class="actions">
												<li><a href="#" class="button">More</a></li>
											</ul>-->
										</article>
									</div>
								</section>
								
							<!-- Section -->
								<section>
									<header class="major">
										<h2>B. 2-speaker samples</h2>
									</header>
									<div class="posts">
										<article>
											<a href="#" class="image"><img src="images/pic02.jpg" alt="" /></a>
											<h3>sample1</h3>
											<p>detail</p>
											<!--<ul class="actions">
												<li><a href="#" class="button">More</a></li>
											</ul>-->
										</article>
									</div>
								</section>

							<!-- Section -->
								<section>
									<header class="major">
										<h2>C. 3-speaker samples</h2>
									</header>
									<div class="posts">
										<article>
											<a href="#" class="image"><img src="images/pic03.jpg" alt="" /></a>
											<h3>sample1</h3>
											<p>detail</p>
											<!--<ul class="actions">
												<li><a href="#" class="button">More</a></li>
											</ul>-->
										</article>
									</div>
								</section>

						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Search
								<section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section>
							-->
							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="index.html">Abstract</a></li>
										<li><a href="generic.html">Dataset</a></li>
										<li>
											<span class="opener">Audio-visual samples</span>
											<ul>
												<li><a href="#">1-speaker samples</a></li>
												<li><a href="#">2-speaker samples</a></li>
												<li><a href="#">3-speaker samples</a></li>
												<li><a href="#">in the wild</a></li>
											</ul>
										</li>
									</ul>
								</nav>

							<!-- Section -->
								<section>
									<header class="major">
										<h2>Contact us</h2>
									</header>
									<p>contact us...</p>
									<ul class="contact">
										<li class="icon solid fa-envelope"><a href="#">1701111335@pku.edu.cn</a></li>
										<li class="icon solid fa-phone">00000000</li>
										<li class="icon solid fa-home">???????<br />
										??????</li>
									</ul>
								</section>

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; Untitled. All rights reserved. Demo Images: <a href="https://unsplash.com">Unsplash</a>. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>